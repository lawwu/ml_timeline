# ML News

## May 2023

* 10: Transformers Agents from HuggingFace ([tweet](https://twitter.com/huggingface/status/1656334778407297027), [docs](https://huggingface.co/docs/transformers/transformers_agents))
* 9: Language models can explain neurons in language models. ([tweet](https://twitter.com/janleike/status/1655982055736643585), [post](https://openai.com/research/language-models-can-explain-neurons-in-language-models))
* 5: Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs ([post](https://www.mosaicml.com/blog/mpt-7b))
* 4: StarCoder: A State-of-the-Art LLM for Code ([post](https://huggingface.co/blog/starcoder))
* 3: OpenLLaMA: An Open Reproduction of LLaMA ([post](https://huggingface.co/openlm-research/open_llama_7b_preview_300bt))

## April 2023
* 25: HuggingChat: Making the best open source AI chat models available to everyone. ([site](https://huggingface.co/chat/), [tweet](https://twitter.com/julien_c/status/1650884245823320065))
* 19: StableLM: Stability AI's first LLM. They released 3B and 7B param models, with 15-65B to follow. ([tweet](https://twitter.com/StabilityAI/status/1648706156330876928))
* 17: RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens
 ([post](https://www.together.xyz/blog/redpajama), [dataset](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T))
* 17: Generative Agents paper's (Park et all) long-term, reflection based memory implemented in langchain ([tweet](https://twitter.com/hwchase17/status/1647987713449263106))
* 16: CAMEL: Physics, Chemistry and Biology datasets ([tweet](https://twitter.com/hammh0a/status/1647415963644760064), but @jeremyphoward [points out the evaluation is a little sketchy](https://twitter.com/jeremyphoward/status/1647714442355015680))
* 15: OpenAssistant released along with 600k human generated datapoints ([tweet](https://twitter.com/ykilcher/status/1647283816384405505), )
* 15: ControlNet 1.1 ([tweet](https://twitter.com/huggingface/status/1647017924459126784))
* 12: DeepSpeed Chat - DeepSpeed Chat offers an end-to-end RLHF pipeline to train ChatGPT-like models. This is the missing piece from other efforts like Alpaca and Vicuna. The RLHF pipeline is replicated from the InstructGPT paper. ([tweet](https://twitter.com/omarsar0/status/1645936415941836804))
* 12: Dolly 2.0 - Introducing the first *commercially viable*, open source, instruction-following LLM. Dolly 2.0 is available for commercial applications without having to pay for API access or sharing data with 3rd parties. ([post](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm), [tweet](https://twitter.com/alighodsi/status/1646151697415168006))
* 9: baby GPT from Andrej Karpathy "I think it's interesting to train/study tiny GPTs because it becomes tractable to visualize and get an intuitive sense of the entire dynamical system." ([tweet](https://twitter.com/karpathy/status/1645115622517542913))
* 9: Generative Agents: Interactive Simulacra of Human Behavior ([paper](https://arxiv.org/abs/2304.03442))
* 7: SegGPT: Segmenting Everything in context ([tweet](https://twitter.com/_akhaliq/status/1644147931178496001))
* 6: Vicuna-7B weights are released ([tweet](https://twitter.com/lmsysorg/status/1644060638472470528))
* 6: StackLlama ([tweet](https://twitter.com/lvwerra/status/1643998302738759683))
* 6: VideoCrafter: text to video model ([tweet](https://twitter.com/TomLikesRobots/status/1643878218498207744))
* 6: Generative Novel View synthesis ([tweet](https://twitter.com/_akhaliq/status/1643790003779059715))
* 5: SAM - Segment anything ([tweet](https://twitter.com/MetaAI/status/1643599800414380038))
* 5: ChatArena, multi-agent game environments for LLMs ([tweet](https://twitter.com/mindjimmy/status/1643633046208249856))
* 5: Kandinsky 2.1 for image generation ([tweet](https://twitter.com/nearcyan/status/1643421466795417600))
* 5: LLaMA-Adapter ([tweet](https://twitter.com/lupantech/status/1643385891338227712))
* 5: LatentVideo Diffusion Models for long video generation ([tweet](https://twitter.com/_akhaliq/status/1643627527594815488))
* 4: Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models ([paper](https://arxiv.org/abs/2304.01852)) 
* 4: MolFeat, a hub of molecular featurizers ([tweet](https://twitter.com/datamol_io/status/1643263399915311104))
* 4: LangChain announced their $10M seed round ([tweet](https://twitter.com/hwchase17/status/1643301144717066240))
* 4: Kandinsky 2.1 ([tweet](https://twitter.com/_akhaliq/status/1643191350672646144))
* 4: IGEL, an instruction-uned German LLM ([tweet](https://twitter.com/_philschmid/status/1643278444992626689))
* 4: Koala-13B: A Dialogue Model for Academic Research ([tweet](https://twitter.com/AlphaSignalAI/status/1643306708716904461), [blog](https://bair.berkeley.edu/blog/2023/04/03/koala/))
* 4: Baize: An Open-Source chat model with PEFT ([tweet](https://twitter.com/arankomatsuzaki/status/1643054506148614146))
* 3: Vicuna-13B weights are released ([tweet](https://twitter.com/lmsysorg/status/1642968294998306816))
* 3: A Survey of Large Language Models ([tweet](https://twitter.com/arankomatsuzaki/status/1642686213147738112))
* 3: babyagi - Open-sourcing “Baby AGI”, a paired down version of the “Task-Driven Autonomous Agent” at 105 lines of code. ([tweet](https://twitter.com/yoheinakajima/status/1642881722495954945))


## March 2023
* 30: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace ([paper](https://arxiv.org/abs/2303.17580), [code](https://github.com/microsoft/JARVIS)) - Using a LLM as brain, HuggingGPT identifies what HuggingFace models to use to solve tasks. Notably Microsoft is calling this `JARVIS`
* 30: Ethics and Society Newsletter #3: Ethical Openness at Hugging Face - ([post](https://huggingface.co/blog/ethics-soc-3))
* 30: BloombergGPT: A Large Language Model for Finance ([tweet](https://twitter.com/TechAtBloomberg/status/1641772329658114053), [paper](https://arxiv.org/abs/2303.17564))
* 30: Nucleotide Transformer, SOTA Genomics ([tweet](https://twitter.com/instadeepai/status/1641075963051012097), [code](https://github.com/instadeepai/nucleotide-transformer))
* 30: ColossalChat ([blog](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b), [code](https://github.com/hpcaitech/ColossalAI))
* 29: GeoV-9b ([tweet](https://twitter.com/labmlai/status/1641357802009395201), [code](https://github.com/geov-ai/geov), [weights](https://huggingface.co/GeoV/GeoV-9b), [colab](https://colab.research.google.com/github/geov-ai/geov/blob/master/notebooks/generate.ipynb))
* 29: Spanish BERTIN GPT-J-6B Alpaca and Alpaca LoRA ([tweet](https://twitter.com/versae/status/1641124547414900736))
* 29: LLaMA Adapter ([tweet](https://twitter.com/lupantech/status/1640899600281395200), [code](https://github.com/ZrrSkywalker/LLaMA-Adapter), [paper](https://huggingface.co/papers/2303.16199))
* 28: PRESTO dataset ([github](https://github.com/google-research-datasets/presto))
* 28: OpenFlamingo ([tweet](https://twitter.com/anas_awadalla/status/1640766789977251840), [blog](https://laion.ai/blog/open-flamingo/))
* 28: Raven RWKV (RWKV finetuned on alpaca and codealpaca) ([tweet](https://twitter.com/BlinkDL_AI/status/1640742627216875524), [demo](https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B))
* 28: Cerebras-GPT ([tweet](https://twitter.com/CerebrasSystems/status/1640725880711569408), [models](https://huggingface.co/cerebras))
* 28: GPT4All ([tweet](https://twitter.com/andriy_mulyar/status/1640836003194630144), [code](https://github.com/nomic-ai/gpt4all))
* 28: Replit Partners with Google Cloud ([tweet](https://twitter.com/Replit/status/1640745029080866817))
* 27: LLaMA voice chat + Siri TTS ([tweet](https://twitter.com/ggerganov/status/1640416314773700608))
* 27: GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models ([paper](https://arxiv.org/pdf/2303.10130.pdf)) – Paper that identifies the occupations that have the highest exposure to automation by GPT. In related news, ResumeBuilder found [1 in 4 companies have already replaced workers with ChatGPT](https://www.resumebuilder.com/1-in-4-companies-have-already-replaced-workers-with-chatgpt/#:~:text=With%20the%20emergence%20of%20ChatGPT,%2C%20write%20code%2C%20and%20more)
* 26: Japanese Alpaca LoRA ([tweet](https://twitter.com/kun1em0n/status/1639965140429963264), [demo](https://huggingface.co/spaces/kunishou/Japanese-Alpaca-LoRA-7b-DEMO), [report](https://note.com/kun1emon/n/n1533345d5d26))
* 26: LLaMA voice chat ([tweet](https://twitter.com/ggerganov/status/1640022482307502085))
* 24: Text2Video-Zero ([tweet](https://twitter.com/_akhaliq/status/1639062868850266112), [code](https://github.com/Picsart-AI-Research/Text2Video-Zero))
* 24: Dolly ([tweet](https://twitter.com/databricks/status/1639239800145465344), [code](https://github.com/databrickslabs/dolly), [demo](https://huggingface.co/databricks/dolly-v1-6b))
* 22: Alpaca LoRA as a chatbot ([tweet](https://twitter.com/algo_diver/status/1638525828773576704), [code](https://github.com/deep-diver/Alpaca-LoRA-Serve)).
* 20: Reflexion: an autonomous agent with dynamic memory and self-reflection ([paper](https://arxiv.org/abs/2303.11366)). A related [post](https://nanothoughts.substack.com/p/reflecting-on-reflexion). 
* 20: Runway Gen-2 ([tweet](https://twitter.com/runwayml/status/1637800500459458562))
* 20: [GPT 4 and the Uncharted Territories of Language](https://www.fast.ai/posts/2023-03-20-wittgenstein.html) - Jeremy Howard uses GPT-4 to write a blog post highlighting some of the dangers of language models. He used 4 prompts to write the post.
* 24: SwissBERT ([tweet](https://twitter.com/j_vamvas/status/1639192870828556290), [blog](https://vamvas.ch/introducing-swissbert))
* 17: Alpacoom: BLOOM fine-tuned on Alpaca's dataset using LoRA ([tweet](https://twitter.com/mrm8488/status/1636742703055527937?s=20), [model](https://huggingface.co/mrm8488/Alpacoom))
* 16: Alpaca LoRA: instruct tune LLAMA on consumer hardware ([tweet](https://twitter.com/_akhaliq/status/1636416647518097408), [code](https://github.com/tloen/alpaca-lora))
* 15: GPT-4 Technical Paper ([paper](https://arxiv.org/abs/2303.08774)) - highlights some of the amazing improvements GPT-4 has made over GPT-3
* 13: [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) – Stanford’s CRFM group released a 1.5B parameter GPT-3 like model. They were the first to demonstrate you can get GPT-like performance using only 52k instruction-following data points. On the self-instruct evaluation set, Alpaca shows many behaviors similar to OpenAI’s text-davinci-003, but is also surprisingly small and easy/cheap to reproduce. I think one reason OpenAI dropped their pricing by 90% with GPT-4 is because they wanted to achieve wide distribution of their model.

## August 2022
* 21: Emergent Abilities of Large Language Models ([paper](https://openreview.net/forum?id=yzkSU5zdwD), [blog](https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html))

## July 2022
* 20: Donut released - OCR-free Document Understanding Transformer (Donut) and Synthetic Document Generator (SynthDoG) ([repo](https://github.com/clovaai/donut))